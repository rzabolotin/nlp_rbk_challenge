{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e9bb743",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import time\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import re\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1b58951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install selenium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc9ef5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_dataset_train.csv')\n",
    "test = pd.read_csv('test_dataset_test.csv')\n",
    "data = pd.concat([train, test])\n",
    "ltr = len(train)\n",
    "data = data.reset_index(drop = True)\n",
    "data['tags'] = data['tags'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "60288df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test])\n",
    "ltr = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf9e6592",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "11650773",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['tags'] = data['tags'].apply(ast.literal_eval)\n",
    "data['authors'] = data['authors'].apply(ast.literal_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf9f2f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(np.concatenate(data['authors'].tolist()))\n",
    "data['vc_authors'] = [sum([cnt[x] for x in a]) for a in data['authors']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1fb8b08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt = Counter(np.concatenate(data['tags'].tolist()))\n",
    "data['vc_tags'] = [sum([cnt[x] for x in a]) for a in data['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cc66fda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['len_tags'] = [len(a) for a in data['tags']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "56098dce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('list_data_new_1.pickle', 'rb') as f:\n",
    "    text_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a894a21d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['site'] = [x[0] for x in text_data]\n",
    "data['overview'] = [x[2] for x in text_data]\n",
    "data['photo'] = [x[3] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9f2eab96",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['header'] = [x[1].split(',')[0] for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3cd9dc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_text'] = [' '.join(x[-1][:[i for i, x in enumerate(x[-1]) if x == ''][0]]) \n",
    "    if len( [i for i, x in enumerate(x[-1]) if x == '']) else ' '.join(x[-1]) for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3f22f858",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['full_text_d'] = [' '.join(x[4]) for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b7825c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['first_sent_text_d'] = [x[4][0] if len(x[4]) else '' for x in text_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2192ec49",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['publish_date'] = pd.to_datetime(data['publish_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "385a940a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['feat_days'] = (data['publish_date'] - pd.to_datetime('2017-01-01') ).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c702d2cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['hours'] = data['publish_date'].dt.hour * 60 + data['publish_date'].dt.minute\n",
    "data['weekday'] = data['publish_date'].dt.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e6dff6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['day'] = data['publish_date'].dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "13810a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['clean_title'] = [x.split('\\n')[0] for x in data['title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34c19f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vc_header'] = data['header'].map(data['header'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "3e445a47",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:261: RuntimeWarning: Degrees of freedom <= 0 for slice\n",
      "  ret = _var(a, axis=axis, dtype=dtype, out=out, ddof=ddof,\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:221: RuntimeWarning: invalid value encountered in true_divide\n",
      "  arrmean = um.true_divide(arrmean, div, out=arrmean, casting='unsafe',\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "data['len_clean_title'] = data['clean_title'].apply(lambda x:len(x))\n",
    "data['len_clean_title_d'] = data['title'].apply(lambda x:len(x))\n",
    "data['len_full_text'] = data['full_text'].apply(lambda x:len(x))\n",
    "data['len_full_text_d'] = data['full_text_d'].apply(lambda x:len(x))\n",
    "\n",
    "data['len_title'] = data['title'].apply(lambda x:len(x))\n",
    "data['len_authors'] = data['authors'].apply(lambda x:len(x))\n",
    "data['len_tags'] = data['tags'].apply(lambda x:len(x))\n",
    "data['len_photo'] = data['photo'].apply(lambda x:len(x))\n",
    "data['len_overview'] = data['overview'].apply(lambda x:len(x))\n",
    "data['all_len'] = data['len_clean_title'] + data['len_overview']\n",
    "data['len_numbers_title'] = [len(re.findall('\\d+', x)) for x in data['clean_title']]\n",
    "data['len_english_title'] = [len(re.findall('[A-z]', x)) for x in data['clean_title']]\n",
    "data['len_dots_title'] = [len(x.split('. ')) for x in data['clean_title']]\n",
    "data['len_english_text'] = [len(re.findall('[A-z]', x)) for x in data['full_text']]\n",
    "data['len_numbers_text'] = [len(re.findall('\\d+', x)) for x in data['full_text']]\n",
    "data['len_dots_text'] = [len(x.split('. ')) for x in data['full_text']]\n",
    "data['len_overview'] = data['overview'].apply(lambda x:len(x))\n",
    "data['len_sites_split'] = [len(x.split('/')) for x in data['site']]\n",
    "\n",
    "data['len_parse'] = [len(x[4]) for x in text_data]\n",
    "\n",
    "data['abzac_full_text'] = [len( [i for i, x in enumerate(x[-1]) if x == '']) for x in text_data]\n",
    "data['abzac_full_text0'] = [np.std( [len(x) for i, x in enumerate(x[-1]) if x != '']) for x in text_data]\n",
    "# session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e4b8f4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 1000)\n",
    "tfidf_data = tfidf.fit_transform(data['clean_title'] + ' ' + data['overview'])\n",
    "data_tfidf = pd.DataFrame(tfidf_data.toarray())\n",
    "data_tfidf.columns = [f'tfidf_title_{x}' for x in data_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "66b9356a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf_fs = TfidfVectorizer(max_features = 1000)\n",
    "# tfidf_data_fs = tfidf_fs.fit_transform(data['first_sent_text_d'])\n",
    "# data_tfidf_fs = pd.DataFrame(tfidf_data_fs.toarray())\n",
    "# data_tfidf_fs.columns = [f'tfidf_title_fs_{x}' for x in data_tfidf_fs.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a2fb2d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text = TfidfVectorizer(max_features = 5000)\n",
    "tfidf_data_text = tfidf_text.fit_transform(data['full_text'])\n",
    "data_text_tfidf = pd.DataFrame(tfidf_data_text.toarray())\n",
    "data_text_tfidf.columns = [f'tfidf_text_{x}' for x in data_text_tfidf.columns]\n",
    "\n",
    "tfidf_text_d = TfidfVectorizer(max_features = 5000)\n",
    "tfidf_data_d_text = tfidf_text_d.fit_transform(data['full_text_d'])\n",
    "data_text_tfidf_d = pd.DataFrame(tfidf_data_d_text.toarray())\n",
    "data_text_tfidf_d.columns = [f'tfidf_text_d_{x}' for x in data_text_tfidf_d.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f4b3362a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['vc_category'] = data['category'].map(data['category'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "998dcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_tags = TfidfVectorizer(max_features = 100)\n",
    "tfidf_tags_data = tfidf_tags.fit_transform(data['tags'].apply(lambda x: ' '.join(x)))\n",
    "data_tags_tfidf = pd.DataFrame(tfidf_tags_data.toarray())\n",
    "data_tags_tfidf.columns = [f'tfidf_tags_{x}' for x in data_tags_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "97a60893",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_aut = TfidfVectorizer(min_df = 10)\n",
    "tfidf_aut_data = tfidf_aut.fit_transform(data['authors'].apply(lambda x: ' '.join(x)))\n",
    "data_aut_tfidf = pd.DataFrame(tfidf_aut_data.toarray())\n",
    "data_aut_tfidf.columns = [f'tfidf_aut_{x}' for x in data_aut_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c3dceacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_photo = TfidfVectorizer(min_df = 20)\n",
    "tfidf_photo_text = tfidf_photo.fit_transform(data['feat_days'].astype('str'))\n",
    "data_photo_tfidf = pd.DataFrame(tfidf_photo_text.toarray())\n",
    "data_photo_tfidf.columns = [f'tfidf_photo_{x}' for x in data_photo_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd872f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_head = TfidfVectorizer(min_df = 10)\n",
    "tfidf_head_data = tfidf_head.fit_transform(data['header'])\n",
    "data_head_tfidf = pd.DataFrame(tfidf_head_data.toarray())\n",
    "data_head_tfidf.columns = [f'tfidf_head_{x}' for x in data_head_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e72dffa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_cat = TfidfVectorizer(min_df = 10)\n",
    "tfidf_cat_data = tfidf_cat.fit_transform(data['category'])\n",
    "data_cat_tfidf = pd.DataFrame(tfidf_cat_data.toarray())\n",
    "data_cat_tfidf.columns = [f'tfidf_cat_{x}' for x in data_cat_tfidf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f566fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD \n",
    "pca = TruncatedSVD(10, random_state = 121)\n",
    "pca_cat_data = pca.fit_transform(tfidf_data)\n",
    "data_cat_pca = pd.DataFrame(pca_cat_data)\n",
    "data_cat_pca.columns = [f'pca_cat_{x}' for x in data_cat_pca.columns]\n",
    "\n",
    "pca = TruncatedSVD(15, random_state = 121)\n",
    "pca_cat_data = pca.fit_transform(tfidf_tags_data)\n",
    "data_fs_pca = pd.DataFrame(pca_cat_data)\n",
    "data_fs_pca.columns = [f'pca_fs_{x}' for x in data_fs_pca.columns]\n",
    "\n",
    "pca = TruncatedSVD(20, random_state = 121)\n",
    "# pca = TruncatedSVD(20, random_state = 311)\n",
    "pca_text_data = pca.fit_transform(tfidf_data_text)\n",
    "data_text_pca = pd.DataFrame(pca_text_data)\n",
    "data_text_pca.columns = [f'pca_text_{x}' for x in data_text_pca.columns]\n",
    "\n",
    "pca = TruncatedSVD(25, random_state = 121)\n",
    "# pca = TruncatedSVD(20, random_state = 311)\n",
    "pca_text_data_d = pca.fit_transform(tfidf_data_d_text)\n",
    "data_text_pca_d = pd.DataFrame(pca_text_data_d)\n",
    "data_text_pca_d.columns = [f'pca_textd_{x}' for x in data_text_pca_d.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "007e00e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\AppData\\Local\\Temp/ipykernel_3084/2904110049.py:1: FutureWarning: In a future version of pandas all arguments of concat except for the argument 'objs' will be keyword-only\n",
      "  data = pd.concat([data, data_tfidf, data_tags_tfidf, data_text_tfidf, data_aut_tfidf, data_fs_pca, data_head_tfidf, data_cat_tfidf, data_cat_pca, data_text_pca, data_text_pca_d, data_photo_tfidf], 1)\n"
     ]
    }
   ],
   "source": [
    "data = pd.concat([data, data_tfidf, data_tags_tfidf, data_text_tfidf, data_aut_tfidf, data_fs_pca, data_head_tfidf, data_cat_tfidf, data_cat_pca, data_text_pca, data_text_pca_d, data_photo_tfidf], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3337b33c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\numpy\\core\\fromnumeric.py:3419: RuntimeWarning: Mean of empty slice.\n",
      "  return _methods._mean(a, axis=axis, dtype=dtype,\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\numpy\\core\\_methods.py:188: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "data['svo'] = [1 if 'видео' in x.lower() else 0 for x in data['full_text'] ]\n",
    "data['dates'] = data.publish_date.dt.date\n",
    "data['vc_dates'] = data['dates'].map(data['dates'].value_counts())\n",
    "data['ctr_mean'] = data['dates'].map( data[data.ctr != 0].groupby('dates').ctr.mean() )\n",
    "data['ctr_diff'] = data['ctr'] - data['ctr_mean']\n",
    "\n",
    "data['link'] = [x[0][:-len(x[1])] for x in data[['document_id', 'session']].values]\n",
    "data['tmp1'] = [len(re.findall('AP', x)) for x in data['photo']]\n",
    "data['tmp2'] = [len(re.findall('Instagrams', x)) for x in data['photo']]\n",
    "\n",
    "data['abzac_full_text2'] = [np.max( [len(x) for i, x in enumerate(x[-1]) ]) if len(x[-1]) else 0 for x in text_data]\n",
    "data['abzac_full_text3'] = [len(x[-1][0]) if len(x[-1]) else 0  for x in text_data]\n",
    "data['abzac_full_text4'] = [len(x[-1][1]) if len(x[-1]) > 1 else 0  for x in text_data]\n",
    "# data['abzac_full_text4'] = data['abzac_full_text3'] - data['len_parse']\n",
    "\n",
    "data['len_diff1'] = data['len_clean_title'] / data['len_clean_title_d']\n",
    "data['len_diff2'] = data['len_dots_text'] / data['len_full_text']\n",
    "data['len_diff3'] = data['len_dots_title'] / data['len_clean_title']\n",
    "data['len_diff4'] = data['len_full_text'] / data['len_full_text_d']\n",
    "data['len_diff5'] = data['len_full_text'] - data['len_full_text_d']\n",
    "data['len_diff6'] = data['len_title'] / data['len_overview']\n",
    "\n",
    "dict_mean_ctr = {}\n",
    "for ctr, a_l in data[['ctr', 'tags']].values:\n",
    "    if ctr != 0:\n",
    "        for a in a_l:\n",
    "            dict_mean_ctr[a] = dict_mean_ctr.get(a, []) + [ctr]\n",
    "data['ctr_mean_tags'] = [np.mean([np.mean(dict_mean_ctr.get(x, [1])) for x in a]) for a in data['tags']]\n",
    "\n",
    "dict_mean_ctr = {}\n",
    "for ctr, a_l in data[['ctr', 'authors']].values:\n",
    "    if ctr != 0:\n",
    "        for a in a_l:\n",
    "            dict_mean_ctr[a] = dict_mean_ctr.get(a, []) + [ctr]\n",
    "data['ctr_mean_authors'] = [np.mean([np.mean(dict_mean_ctr.get(x, [1])) for x in a]) for a in data['authors']]\n",
    "\n",
    "data['dum1'] = data[[col for col in data.columns if 'tfidf_title_' in col]].sum(1)\n",
    "data['dum2'] = data[[col for col in data.columns if 'tfidf_head_' in col]].sum(1)\n",
    "data['dum3'] = data[[col for col in data.columns if 'tfidf_aut_' in col]].sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb269d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('list_data_new_full_test.pickle', 'rb') as f:\n",
    "    full_text_data = pickle.load(f)\n",
    "    \n",
    "feat_data = []\n",
    "for x in full_text_data:\n",
    "    wer = [i[0] for i in x[1]]\n",
    "    tmp_ind = []\n",
    "    tmp = []\n",
    "    for i, w in enumerate(wer):\n",
    "        if w == '':\n",
    "            if len(tmp):\n",
    "                tmp_ind += [tmp]\n",
    "            tmp = []\n",
    "        else:\n",
    "            tmp += [i]\n",
    "    if len(tmp):\n",
    "        tmp_ind += [tmp]\n",
    "    lens = [np.sum([x[1][j][1][0] for j in i]) for i in tmp_ind]\n",
    "    if lens:\n",
    "        m = np.max(lens)\n",
    "    else:\n",
    "        m = -1\n",
    "    l = len(lens)\n",
    "    lens = lens[:4]\n",
    "    lens += [-1] * (4 - len(lens))\n",
    "    feat_data += [lens + [l,m]]\n",
    "    \n",
    "feat_data = np.array(feat_data)\n",
    "for i in range(6):\n",
    "    data[f'nre_{i}'] = feat_data[:, i]\n",
    "data.loc[:, 'new1'] = [x[0][1][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new2'] = [x[0][0][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new3'] = [x[0][2][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new4'] = [x[0][3][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new5'] = [x[0][4][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new6'] = [x[0][5][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new7'] = [x[0][6][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new8'] = [x[0][7][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new9'] = [x[0][8][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new10'] = [x[0][9][1][0] for x in full_text_data]\n",
    "data.loc[:, 'new12'] = [x[1][0][1][0] if len(x[1]) else -1 for x in full_text_data]\n",
    "data.loc[:, 'new11'] = data['new12'] / data['new1']\n",
    "\n",
    "\n",
    "data.loc[:, 'lnew1'] = [len(x[0][0][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew2'] = [len(x[0][1][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew3'] = [len(x[0][2][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew4'] = [len(x[0][3][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew5'] = [len(x[0][4][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew6'] = [len(x[0][5][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew7'] =[len(x[0][6][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew8'] = [len(x[0][7][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew9'] = [len(x[0][8][0]) for x in full_text_data]\n",
    "data.loc[:, 'lnew10'] = [len(x[0][9][0]) for x in full_text_data]\n",
    "\n",
    "\n",
    "data.loc[:, 'compy1'] = [(np.array([i[1][0] for i in x[1] if 'фото' not in i[0].lower()])).sum() for x in full_text_data]\n",
    "data.loc[:, 'compy2'] = [((np.array([i[1][0] for i in x[1]])) == 0).sum() for x in full_text_data]\n",
    "data['compy3'] = data['compy1'] / data['new1']\n",
    "\n",
    "data.loc[: ,'compy4'] = [(np.array([i[1][0] for i in x[1] ])).sum() for x in full_text_data]\n",
    "data.loc[:, 'compy5'] = [(np.array([i[1][0] for i in x[1] if 'фото' in i[0].lower()])).sum() for x in full_text_data]\n",
    "\n",
    "data['compy6'] = data['compy5'] / data['new1']\n",
    "data['compy7'] = data['compy4'] / data['new1']\n",
    "data['compy8'] = data['compy2'] / data['new1']\n",
    "\n",
    "data['num_abzac'] = feat_data[:, -1]\n",
    "data['compy'] = [len(re.findall( '\\\\n', x[0][1][0] )) for x in full_text_data]\n",
    "data['diff1'] = data['len_overview'] / data['len_full_text'] \n",
    "data['diff2'] = feat_data[:, 0] / data['new1'] \n",
    "data['diff3'] = data['compy'] / data['len_full_text'] \n",
    "data['dr'] = data['new1'] / data['len_full_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b02ff1b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lgb_train(data, target, ltr, split_list, param, v_e = 0, n_e = 5000, cat_col = None):\n",
    "    pred = pd.DataFrame()\n",
    "    pred_val = np.zeros(len(data))\n",
    "    fi = np.zeros(data.shape[1])\n",
    "    score = []\n",
    "    bst_list = []\n",
    "    pred_train = pd.DataFrame()\n",
    "    j = 0\n",
    "    target = target \n",
    "     \n",
    "    full_tr =lgb.Dataset(np.array(data)[:ltr],np.array(target)[:ltr])\n",
    "    dict_score = lgb.cv(param, full_tr, 5000, split_list, early_stopping_rounds=300, verbose_eval = 100)\n",
    "    best_tree = np.argmin(dict_score['l2-mean']) + 1\n",
    "    print(np.min(dict_score['l2-mean']))\n",
    "#     best_tree = 768\n",
    "    for i , (train_index, test_index) in enumerate(split_list):\n",
    "#         drop_index = data[]\n",
    "        tr = lgb.Dataset(np.array(data)[train_index],np.array(target)[train_index] , reference=full_tr)\n",
    "        te = lgb.Dataset(np.array(data)[test_index], np.array(target)[test_index], reference=tr)\n",
    "        evallist = [(tr, 'train'), (te, 'test')]\n",
    "        \n",
    "#         bst = lgb.train(param, tr, num_boost_round=n_e, \n",
    "#                 valid_sets = [tr, te], early_stopping_rounds=int(5 / param['learning_rate']), verbose_eval = v_e)\n",
    "        bst = lgb.train(param, tr, num_boost_round=best_tree)\n",
    "#         return bst\n",
    "        pred[str(i)] = bst.predict(np.array(data)[ltr:, :])\n",
    "        pred_val[test_index] = bst.predict(np.array(data)[test_index]) \n",
    "        score += [r2_score(np.array(target)[test_index] , pred_val[test_index])]\n",
    "        bst_list += [bst]\n",
    "        fi += np.array(bst.feature_importance(importance_type = 'gain'))\n",
    "        print(i+1, np.mean(score))\n",
    "        print()\n",
    "        del tr, te\n",
    "    pred_train[str(j)] = pred_val\n",
    "    return bst_list, pred_train, pred, score, fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1888bb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ltr = len(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c01dd538",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data['link'] + '_' + data['ctr'].astype('str')\n",
    "tmp_vc = tmp.value_counts()\n",
    "have_indexs = tmp[tmp.isin(tmp_vc[tmp_vc >= 2].index)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3994ee2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "def standart_split(data, ltr):\n",
    "    split_list = []\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = 228)\n",
    "    for train_index, test_index in kf.split(data.loc[:ltr-1, :], data.loc[:ltr-1, 'views']):\n",
    "        test_index = sorted(set(test_index) - set(have_indexs))\n",
    "        split_list += [(train_index, test_index)]\n",
    "\n",
    "    return split_list\n",
    "split_list = standart_split(data, ltr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2ec4ce5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data[data[ 'publish_date'] > '2021-01-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9024bd45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import r2_score, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "8f8541af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.mean(score_1) * 0.4 + np.mean(score_2) * 0.3 +  + np.mean(score_3) * 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6f987961",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb_view = {\n",
    "# 'bagging_fraction': 0.9,\n",
    "# 'bagging_freq': 1,\n",
    "'boost': 'gbdt',\n",
    "#     'max_bin' : 500,\n",
    "#     'linear_lambda' : 1,\n",
    "# 'feature_fraction': 1,\n",
    "'learning_rate': 0.1,\n",
    "'metric':'l2',\n",
    "'num_leaves': 12,\n",
    "        'verbosity':-1,\n",
    "'objective': 'l2',\n",
    "        'lambda_l2':1,\n",
    "    'reg_sqrt':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "8768a959",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l2: 1.59414e+09 + 1.8868e+09\n",
      "[200]\tcv_agg's l2: 1.56794e+09 + 1.81787e+09\n",
      "[300]\tcv_agg's l2: 1.56586e+09 + 1.8019e+09\n",
      "[400]\tcv_agg's l2: 1.55881e+09 + 1.78464e+09\n",
      "[500]\tcv_agg's l2: 1.5636e+09 + 1.78497e+09\n",
      "[600]\tcv_agg's l2: 1.56164e+09 + 1.77243e+09\n",
      "[700]\tcv_agg's l2: 1.56482e+09 + 1.77303e+09\n",
      "1557197835.2419212\n",
      "1 0.628327480252117\n",
      "\n",
      "2 0.6268876589612847\n",
      "\n",
      "3 0.5659862266275726\n",
      "\n",
      "4 0.5809379635128021\n",
      "\n",
      "5 0.5766657065384966\n",
      "\n",
      "6 0.5582551274924726\n",
      "\n",
      "7 0.5500023740581373\n",
      "\n",
      "8 0.561061097865258\n",
      "\n",
      "9 0.5669770024000025\n",
      "\n",
      "10 0.557229334016876\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cols_view = ['ctr', 'vc_authors', 'vc_tags', 'feat_days', 'hours',  'day', 'vc_header', \n",
    "                   'len_clean_title_d', 'len_full_text', 'len_authors', 'len_tags',  'len_overview',\n",
    "                   'all_len', 'len_numbers_title', 'len_dots_title', 'len_english_text', \n",
    "                   'len_numbers_text', 'len_dots_text', 'len_sites_split', 'len_diff1', \n",
    "                   'len_parse', 'abzac_full_text', 'abzac_full_text0', 'vc_category', 'svo', 'vc_dates', 'ctr_mean',\n",
    "                    'tmp1',   'nre_5', 'abzac_full_text4', 'lnew5', 'new5', 'dum2', 'compy8',\n",
    "                   'lnew1', 'new10', 'new9', 'compy1', 'compy7', 'diff3'   ]\n",
    "\n",
    "\n",
    "train_cols_view += [x for x in data.columns if 'tfidf_cat_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'tfidf_head_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_aut_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'pca_cat_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_textd_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_text_' in x]\n",
    "# train_cols_view += ['svo1']\n",
    "bst_list, pred_train1, pred_views1, score_1, fi = lgb_train(data[train_cols_view], data['views'], ltr, split_list, param_lgb_view, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "cb8d588c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l2: 1.57226e+09 + 1.86785e+09\n",
      "[200]\tcv_agg's l2: 1.52142e+09 + 1.76545e+09\n",
      "[300]\tcv_agg's l2: 1.51441e+09 + 1.73639e+09\n",
      "[400]\tcv_agg's l2: 1.50986e+09 + 1.71975e+09\n",
      "[500]\tcv_agg's l2: 1.51007e+09 + 1.70789e+09\n",
      "[600]\tcv_agg's l2: 1.50802e+09 + 1.69773e+09\n",
      "[700]\tcv_agg's l2: 1.5072e+09 + 1.69086e+09\n",
      "[800]\tcv_agg's l2: 1.50031e+09 + 1.66799e+09\n",
      "[900]\tcv_agg's l2: 1.49915e+09 + 1.66347e+09\n",
      "[1000]\tcv_agg's l2: 1.50161e+09 + 1.6666e+09\n",
      "[1100]\tcv_agg's l2: 1.50346e+09 + 1.67239e+09\n",
      "1498281738.2814255\n",
      "1 0.6315043431797966\n",
      "\n",
      "2 0.6251725983031924\n",
      "\n",
      "3 0.5761676940936707\n",
      "\n",
      "4 0.593729710658726\n",
      "\n",
      "5 0.5921085520591433\n",
      "\n",
      "6 0.5730305376724896\n",
      "\n",
      "7 0.5703987387899493\n",
      "\n",
      "8 0.5828593383151872\n",
      "\n",
      "9 0.5886586987831975\n",
      "\n",
      "10 0.5691426495242544\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cols_view = ['ctr', 'vc_authors', 'vc_tags', 'feat_days', 'hours', 'weekday', 'day', 'vc_header', 'len_clean_title',\n",
    "                   'len_clean_title_d', 'len_full_text', 'len_authors', 'len_tags', 'len_photo', 'len_overview',\n",
    "                   'all_len', 'len_numbers_title', 'len_dots_title', 'len_english_text', \n",
    "                   'len_numbers_text', 'len_dots_text', 'len_sites_split', 'len_diff1', \n",
    "                   'len_parse', 'abzac_full_text', 'abzac_full_text0', 'vc_category', 'svo', 'vc_dates', 'ctr_mean',\n",
    "                   'ctr_diff', 'tmp1',     ]\n",
    "\n",
    "\n",
    "train_cols_view += [x for x in data.columns if 'tfidf_cat_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'tfidf_head_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_aut_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'pca_cat_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_textd_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_text_' in x]\n",
    "# train_cols_view += ['svo1']\n",
    "bst_list, pred_train2, pred_views2, score_1, fi = lgb_train(data[train_cols_view], data['views'], ltr, split_list, param_lgb_view, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c324fdda",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8422580971698007"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ind = sorted(set(range(7000)) - set(have_indexs))\n",
    "data['pred'] = pred_train2['0']\n",
    "data.loc[(data.ctr >= 0) , 'pred'] *= 1.15\n",
    "mean_squared_error(data['views'][comp_ind], data['pred'][comp_ind]) / 1e9\n",
    "r2_score(data['views'][comp_ind].tolist() + data['views'][have_indexs[have_indexs < 7000]].tolist(),\n",
    "        data['pred'][comp_ind].tolist() +  data['views'][have_indexs[have_indexs < 7000]].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f2b44e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70fe0b6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a4565f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb_view = {\n",
    "# 'bagging_fraction': 0.9,\n",
    "# 'bagging_freq': 1,\n",
    "'boost': 'gbdt',\n",
    "#     'max_bin' : 100,\n",
    "#     'linear_lambda' : 1,\n",
    "# 'feature_fraction': 1,\n",
    "'learning_rate': 0.1,\n",
    "'metric':'l2',\n",
    "'num_leaves': 12,\n",
    "        'verbosity':-1,\n",
    "'objective': 'l2',\n",
    "        'lambda_l2':1,\n",
    "    'reg_sqrt':True\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "83d483d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l2: 54.1591 + 29.7559\n",
      "[200]\tcv_agg's l2: 53.4609 + 29.8557\n",
      "[300]\tcv_agg's l2: 53.3588 + 30.006\n",
      "[400]\tcv_agg's l2: 53.3415 + 29.9476\n",
      "[500]\tcv_agg's l2: 53.5366 + 30.0527\n",
      "[600]\tcv_agg's l2: 53.7078 + 30.1013\n",
      "53.267250105549046\n",
      "1 0.6150626495623017\n",
      "\n",
      "2 0.616905085395435\n",
      "\n",
      "3 0.6085259809091391\n",
      "\n",
      "4 0.5995251665374224\n",
      "\n",
      "5 0.605419851613795\n",
      "\n",
      "6 0.5597375083845898\n",
      "\n",
      "7 0.5738678762033386\n",
      "\n",
      "8 0.5763955586297727\n",
      "\n",
      "9 0.5496420831284127\n",
      "\n",
      "10 0.5585570756537601\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cols_view = ['ctr', 'vc_authors', 'vc_tags', 'feat_days', 'hours', 'weekday', 'day', 'vc_header', 'len_clean_title',\n",
    "                   'len_full_text', 'len_authors', 'len_tags', 'len_overview',\n",
    "                   'all_len', 'len_dots_text', 'len_sites_split', 'compy', 'compy1', 'compy3', \n",
    "                   'len_parse', 'abzac_full_text', 'abzac_full_text0', 'vc_category','vc_dates', 'ctr_mean',\n",
    "                   'ctr_diff', 'abzac_full_text2', 'abzac_full_text3', 'len_diff5', \n",
    "                  'len_diff2', 'len_full_text_d',  'len_diff3', 'len_diff4', 'len_diff6', 'len_title',\n",
    "                  'ctr_mean_tags', 'new1',  'new11', 'lnew10', 'new6', 'compy2'                 ]\n",
    "\n",
    "train_cols_view += [x for x in data.columns if 'tfidf_cat_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_tags_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'tfidf_photo_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_aut_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_head_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'pca_cat_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_textd_' in x]\n",
    "# train_cols_view += [x for x in data.columns if 'pca_text_' in x]\n",
    "bst_list, pred_train_c1, pred_ftr1, score_1, fi = lgb_train(data[train_cols_view], data['full_reads_percent'], ltr, split_list, param_lgb_view, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7dbb1bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l2: 54.9609 + 29.4174\n",
      "[200]\tcv_agg's l2: 54.4599 + 29.5317\n",
      "[300]\tcv_agg's l2: 54.3963 + 29.4612\n",
      "[400]\tcv_agg's l2: 54.5199 + 29.9217\n",
      "[500]\tcv_agg's l2: 54.7263 + 30.0142\n",
      "54.357319081124615\n",
      "1 0.6272408172021955\n",
      "\n",
      "2 0.6102544532286869\n",
      "\n",
      "3 0.6030946416982798\n",
      "\n",
      "4 0.5928969262996571\n",
      "\n",
      "5 0.5945018850185886\n",
      "\n",
      "6 0.5507479288655468\n",
      "\n",
      "7 0.5622566005410014\n",
      "\n",
      "8 0.564465914671801\n",
      "\n",
      "9 0.5390426672512186\n",
      "\n",
      "10 0.5478156290706107\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cols_view = ['ctr', 'vc_authors', 'vc_tags', 'feat_days', 'hours', 'weekday', 'day', 'vc_header', 'len_clean_title',\n",
    "                   'len_full_text', 'len_authors', 'len_tags', 'len_overview',\n",
    "                   'all_len', 'len_dots_text', 'len_sites_split',\n",
    "                   'len_parse', 'abzac_full_text', 'abzac_full_text0', 'vc_category','vc_dates', 'ctr_mean',\n",
    "                   'ctr_diff', 'abzac_full_text2', 'abzac_full_text3', 'len_diff5',\n",
    "                  'len_diff2', 'len_full_text_d',  'len_diff3', 'len_diff4', 'len_diff6', 'len_title',\n",
    "                  'ctr_mean_tags']\n",
    "\n",
    "train_cols_view += [x for x in data.columns if 'tfidf_cat_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_tags_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'tfidf_photo_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_aut_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_head_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'pca_cat_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_textd_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_text_' in x]\n",
    "bst_list, pred_train_c2, pred_ftr2, score_1, fi = lgb_train(data[train_cols_view], data['full_reads_percent'], ltr, split_list, param_lgb_view, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3dfaa883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5521421364634166"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ind = sorted(set(range(7000)) - set(have_indexs))\n",
    "data['pred'] =( pred_train_c1['0'] + pred_train_c2['0']) /2\n",
    "data.loc[(data.ctr > 0) , 'pred'] *= 1\n",
    "# mean_squared_error(data['full_reads_percent'][comp_ind], data['pred'][comp_ind]) / 1e9\n",
    "r2_score(data['full_reads_percent'][comp_ind].tolist() + data['full_reads_percent'][have_indexs[have_indexs < 7000]].tolist(),\n",
    "        data['pred'][comp_ind].tolist() +  data['full_reads_percent'][have_indexs[have_indexs < 7000]].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8b284",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "24f0613c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "def standart_split(data, ltr):\n",
    "    split_list = []\n",
    "    kf = KFold(n_splits = 10, shuffle = True, random_state = 228)\n",
    "    for train_index, test_index in kf.split(data.loc[:ltr-1, :], data.loc[:ltr-1, 'views']):\n",
    "        train_index = sorted(set(train_index) - set(have_indexs))\n",
    "        test_index = sorted(set(test_index) - set(have_indexs))\n",
    "        split_list += [(train_index, test_index)]\n",
    "\n",
    "    return split_list\n",
    "split_list_new = standart_split(data, ltr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4603501d",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_lgb = {\n",
    "# 'bagging_fraction': 0.9,\n",
    "# 'bagging_freq': 1,\n",
    "'boost': 'gbdt',\n",
    "# 'feature_fraction': 1,\n",
    "'learning_rate': 0.05,\n",
    "'metric':'l2',\n",
    "#     'linear_tree' : True,\n",
    "'num_leaves': 32,\n",
    "        'verbosity':-1,\n",
    "'objective': 'l2',\n",
    "    'lambda_l2':10,\n",
    "    'reg_sqrt':True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ba3f07f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l2: 0.000653855 + 0.000139511\n",
      "[200]\tcv_agg's l2: 0.000639313 + 0.000138085\n",
      "[300]\tcv_agg's l2: 0.000639525 + 0.000137279\n",
      "[400]\tcv_agg's l2: 0.000641182 + 0.000136494\n",
      "[500]\tcv_agg's l2: 0.000642753 + 0.000136417\n",
      "0.0006391133782469654\n",
      "1 0.7957060139649272\n",
      "\n",
      "2 0.7950799726131901\n",
      "\n",
      "3 0.7777785005928228\n",
      "\n",
      "4 0.7942461786400736\n",
      "\n",
      "5 0.7982651910484987\n",
      "\n",
      "6 0.8040054390772754\n",
      "\n",
      "7 0.8089755372493199\n",
      "\n",
      "8 0.8109735258974926\n",
      "\n",
      "9 0.8138139181697598\n",
      "\n",
      "10 0.8125937549613189\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cols_view = ['ctr', 'vc_authors', 'vc_tags', 'feat_days', 'hours', 'weekday', 'day', 'vc_header', 'len_clean_title',\n",
    "                   'len_clean_title_d', 'len_full_text', 'len_authors', 'len_tags', 'len_photo', 'len_overview',\n",
    "                   'all_len', 'len_numbers_title', 'len_dots_title', 'len_english_text',\n",
    "                   'len_numbers_text', 'len_dots_text', 'len_sites_split', 'len_diff1',  'len_parse',\n",
    "                   'len_parse', 'abzac_full_text', 'abzac_full_text0', 'vc_category', 'vc_dates', 'ctr_mean',\n",
    "                   'ctr_diff', 'abzac_full_text2', 'abzac_full_text3', 'dum1','new1', 'new12', 'new10',\n",
    "                  'len_diff2','len_full_text_d',  'len_diff3', 'len_diff4',  'len_title',\n",
    "                  'new2',  'new5']\n",
    "\n",
    "train_cols_view += [x for x in data.columns if 'tfidf_cat_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_tags_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_aut_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_head_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'pca_cat_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_textd_' in x]\n",
    "# train_cols_view += [x for x in data.columns if 'pca_text_' in x]\n",
    "bst_list, pred_train_d1, pred_depth1, score_1, fi = lgb_train(data[train_cols_view], data['depth'], ltr, split_list_new, param_lgb, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b469bf67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:577: UserWarning: 'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. Pass 'early_stopping()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'early_stopping_rounds' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "C:\\Users\\Sergei\\anaconda3\\lib\\site-packages\\lightgbm\\engine.py:620: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tcv_agg's l2: 0.000658246 + 0.000140839\n",
      "[200]\tcv_agg's l2: 0.000649541 + 0.000140554\n",
      "[300]\tcv_agg's l2: 0.00065099 + 0.000141719\n",
      "[400]\tcv_agg's l2: 0.000652492 + 0.000144515\n",
      "[500]\tcv_agg's l2: 0.000654964 + 0.000146384\n",
      "0.0006491699306970062\n",
      "1 0.8047226970475\n",
      "\n",
      "2 0.8032590840740452\n",
      "\n",
      "3 0.7791135814873197\n",
      "\n",
      "4 0.7935935503175856\n",
      "\n",
      "5 0.7967057154230052\n",
      "\n",
      "6 0.8017236808025899\n",
      "\n",
      "7 0.8063440479430289\n",
      "\n",
      "8 0.808539154648834\n",
      "\n",
      "9 0.8108804174361303\n",
      "\n",
      "10 0.8096182346231394\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_cols_view = ['ctr', 'vc_authors', 'vc_tags', 'feat_days', 'hours', 'weekday', 'day', 'vc_header', 'len_clean_title',\n",
    "                   'len_clean_title_d', 'len_full_text', 'len_authors', 'len_tags', 'len_photo', 'len_overview',\n",
    "                   'all_len', 'len_numbers_title', 'len_dots_title', 'len_english_text',\n",
    "                   'len_numbers_text', 'len_dots_text', 'len_sites_split', 'len_diff1',  'len_parse',\n",
    "                   'len_parse', 'abzac_full_text', 'abzac_full_text0', 'vc_category', 'vc_dates', 'ctr_mean',\n",
    "                   'ctr_diff', 'abzac_full_text2', 'abzac_full_text3', 'dum1',\n",
    "                  'len_diff2','len_full_text_d',  'len_diff3', 'len_diff4', 'len_diff5',  'len_title']\n",
    "\n",
    "train_cols_view += [x for x in data.columns if 'tfidf_cat_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_tags_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_aut_' in x] \n",
    "train_cols_view += [x for x in data.columns if 'tfidf_head_' in x] \n",
    "# train_cols_view += [x for x in data.columns if 'pca_cat_' in x]\n",
    "train_cols_view += [x for x in data.columns if 'pca_textd_' in x]\n",
    "# train_cols_view += [x for x in data.columns if 'pca_text_' in x]\n",
    "bst_list, pred_train_d2, pred_depth2, score_1, fi = lgb_train(data[train_cols_view], data['depth'], ltr, split_list_new, param_lgb, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ab07136f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8516634825052118"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comp_ind = sorted(set(range(7000)) - set(have_indexs))\n",
    "data['pred'] =( pred_train_d1['0'] + pred_train_d2['0']) /2\n",
    "data.loc[(data.ctr > 0) , 'pred'] *= 1.\n",
    "# mean_squared_error(data['full_reads_percent'][comp_ind], data['pred'][comp_ind]) / 1e9\n",
    "r2_score(data['depth'][comp_ind].tolist() + data['depth'][have_indexs[have_indexs < 7000]].tolist(),\n",
    "        data['pred'][comp_ind].tolist() +  data['depth'][have_indexs[have_indexs < 7000]].tolist()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a33229d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e92b47",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ea13ca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a778d491",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = data['link'] + '_' + data['ctr'].astype('str')\n",
    "tmp_vc = tmp.value_counts()\n",
    "have_indexs = tmp[tmp.isin(tmp_vc[tmp_vc >= 2].index)].loc[7000:].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "200d0009",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_dict = {}\n",
    "for ind in have_indexs:\n",
    "    if ind  != 8380:\n",
    "        ans = data[tmp == tmp[ind]].loc[:7000-1, :][['ctr', 'dates', 'views', 'depth', 'full_reads_percent']]\n",
    "        if len(ans):\n",
    "            true_dict[ind] = ans.iloc[0, :].tolist()[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "644a121e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(true_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "266a35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.DataFrame()\n",
    "submit['document_id'] = test['document_id']\n",
    "submit['views'] = (pred_views1.mean(1) + pred_views2.mean(1)) / 2\n",
    "submit['depth'] = (pred_depth1.mean(1) + pred_depth2.mean(1)) / 2\n",
    "submit['full_reads_percent'] = (pred_ftr1.mean(1) + pred_ftr2.mean(1)) / 2\n",
    "submit['views'] = submit['views'] * 1.1\n",
    "\n",
    "submit['views'] = [x if i + 7000 not in true_dict else true_dict[i + 7000][0] for i, x in enumerate(submit['views']) ]\n",
    "submit['depth'] = [x if i + 7000 not in true_dict else true_dict[i + 7000][1] for i, x in enumerate(submit['depth']) ]\n",
    "submit['full_reads_percent'] = [x if i - 7000 not in true_dict else true_dict[i + 7000][2] for i, x in enumerate(submit['full_reads_percent']) ]\n",
    "\n",
    "submit.to_csv('final.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3592ef22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
